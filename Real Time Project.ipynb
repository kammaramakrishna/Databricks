{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "56352f87-28c5-49bb-a8cf-9fd30983d2e9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "Cancelled",
       "errorTraceType": "html",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, to_date, current_timestamp, expr\n",
    "from pyspark.sql.types import StructType, StructField, StringType, DoubleType\n",
    "import datetime\n",
    "\n",
    "blob_container_name = \"shellcsvblob\"\n",
    "blob_storage_account_name = \"shellcpblob\"\n",
    "blob_storage_access_key = \"oUB3Q/2hW5nTqq0hyWeMfLM5PFmGmZ27qp0sIXr6FoR8gO1kT3unKhvOnfEJv6lqYPIw6UUYR1VD+AStntj+2A==\"\n",
    "\n",
    "adls_container_name = \"bronze\"\n",
    "adls_storage_account_name = \"shelladlssa\"\n",
    "adls_storage_access_key = \"O6gxvhRAlCQR67WDve7erSwbDkfvM0TCY0psdehHE+GkbKNKBoezwslKT9+XgxWu0hHkK+mpfDnC+ASt+PFIEg==\"\n",
    "\n",
    "# Configure Spark to access Blob Storage and ADLS\n",
    "spark.conf.set(f\"fs.azure.account.key.{blob_storage_account_name}.blob.core.windows.net\", blob_storage_access_key)\n",
    "spark.conf.set(f\"fs.azure.account.key.{adls_storage_account_name}.dfs.core.windows.net\", adls_storage_access_key)\n",
    "start_date = datetime.date(2024, 8, 1)\n",
    "end_date = datetime.date(2024, 8, 7)\n",
    "\n",
    "# Initialize empty DataFrames for each country\n",
    "us_df_combined = None\n",
    "ind_df_combined = None\n",
    "nl_df_combined = None\n",
    "\n",
    "# Define the schema\n",
    "schema = StructType([\n",
    "    StructField(\"Country_ID\", StringType(), True),\n",
    "    StructField(\"Date\", StringType(), True),  \n",
    "    StructField(\"Site_ID\", StringType(), True),\n",
    "    StructField(\"Product_ID\", StringType(), True),\n",
    "    StructField(\"Volume_Sold\", StringType(), True)  \n",
    "])\n",
    "\n",
    "# Iterate through the date range\n",
    "current_date = start_date\n",
    "while current_date <= end_date:\n",
    "    # Format the current date as string\n",
    "    current_date_str = current_date.strftime('%Y-%m-%d')\n",
    "\n",
    "    # Construct file paths\n",
    "    blob_folder_path = f\"{current_date_str}\"\n",
    "    blob_file_paths = [f\"{blob_folder_path}/{country}.csv\" for country in [\"US\", \"IN\", \"NL\"]]\n",
    "\n",
    "    # Read the CSV files without modifying data\n",
    "    dfs = []\n",
    "    for file_path in blob_file_paths:\n",
    "        df = (spark.read.format(\"csv\")\n",
    "              .option(\"header\", \"true\")\n",
    "              .schema(schema)\n",
    "              .load(f\"wasbs://{blob_container_name}@{blob_storage_account_name}.blob.core.windows.net/{file_path}\"))\n",
    "        \n",
    "        dfs.append(df)\n",
    "\n",
    "    # Combine DataFrames\n",
    "    if dfs:\n",
    "        combined_df = spark.createDataFrame(\n",
    "            [row for df in dfs for row in df.collect()], schema)\n",
    "\n",
    "        combined_df = combined_df.withColumn(\"load_timestamp\", current_timestamp())\n",
    "\n",
    "        # Pivot the combined table to create the final tables\n",
    "        if us_df_combined is None:\n",
    "            us_df_combined = combined_df.filter(col(\"Country_ID\") == \"USA\")\n",
    "        else:\n",
    "            us_df_combined = us_df_combined.union(combined_df.filter(col(\"Country_ID\") == \"USA\"))\n",
    "\n",
    "        if ind_df_combined is None:\n",
    "            ind_df_combined = combined_df.filter(col(\"Country_ID\") == \"IND\")\n",
    "        else:\n",
    "            ind_df_combined = ind_df_combined.union(combined_df.filter(col(\"Country_ID\") == \"IND\"))\n",
    "\n",
    "        if nl_df_combined is None:\n",
    "            nl_df_combined = combined_df.filter(col(\"Country_ID\") == \"NL\")\n",
    "        else:\n",
    "            nl_df_combined = nl_df_combined.union(combined_df.filter(col(\"Country_ID\") == \"NL\"))\n",
    "\n",
    "    # Increment the date\n",
    "    current_date += datetime.timedelta(days=1)\n",
    "\n",
    "# Write the final DataFrames as Delta tables\n",
    "if nl_df_combined is not None:\n",
    "    (nl_df_combined\n",
    "     .write\n",
    "     .format(\"delta\")\n",
    "     .mode(\"overwrite\")\n",
    "     .save(f\"abfss://{adls_container_name}@{adls_storage_account_name}.dfs.core.windows.net/combined/nl\"))\n",
    "\n",
    "if us_df_combined is not None and us_df_combined.count() > 0:\n",
    "    (us_df_combined\n",
    "     .write\n",
    "     .format(\"delta\")\n",
    "     .mode(\"overwrite\")\n",
    "     .save(f\"abfss://{adls_container_name}@{adls_storage_account_name}.dfs.core.windows.net/combined/us\"))\n",
    "\n",
    "if ind_df_combined is not None and ind_df_combined.count() > 0:\n",
    "    (ind_df_combined\n",
    "     .write\n",
    "     .format(\"delta\")\n",
    "     .mode(\"overwrite\")\n",
    "     .save(f\"abfss://{adls_container_name}@{adls_storage_account_name}.dfs.core.windows.net/combined/ind\"))\n",
    "    display(ind_df_combined)\n",
    "    display(us_df_combined)\n",
    "    display(nl_df_combined)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "95e3510b-abf7-4658-ba3d-d90ca90547be",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "Command skipped",
       "errorTraceType": "html",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, row_number, when, desc, to_date, regexp_replace, lit, max as max_\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.types import DoubleType\n",
    "\n",
    "# Define paths to the Delta tables\n",
    "us_delta_path = f\"abfss://{adls_container_name}@{adls_storage_account_name}.dfs.core.windows.net/combined/us\"\n",
    "ind_delta_path = f\"abfss://{adls_container_name}@{adls_storage_account_name}.dfs.core.windows.net/combined/ind\"\n",
    "nl_delta_path = f\"abfss://{adls_container_name}@{adls_storage_account_name}.dfs.core.windows.net/combined/nl\"\n",
    "\n",
    "# Read the Delta tables into DataFrames\n",
    "us_df = spark.read.format(\"delta\").load(us_delta_path)\n",
    "ind_df = spark.read.format(\"delta\").load(ind_delta_path)\n",
    "nl_df = spark.read.format(\"delta\").load(nl_delta_path)\n",
    "\n",
    "\n",
    "combined_df = us_df.union(ind_df).union(nl_df)\n",
    "display(combined_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0d7aab0b-1113-43e7-a5df-716d1adca313",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "Command skipped",
       "errorTraceType": "html",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "bronze_delta_path = f\"abfss://{adls_container_name}@{adls_storage_account_name}.dfs.core.windows.net/bronze_table\"\n",
    "\n",
    "# Write the combined DataFrame to the Bronze layer as a Delta table\n",
    "combined_df.write.format(\"delta\").mode(\"overwrite\").save(bronze_delta_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c7261e43-b72a-46c7-845d-24765a93338e",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Reference Tables for date, country code, global product id, global product category, global site id\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d6145ec9-8a4a-4604-84fb-ea994ba4965f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "Command skipped",
       "errorTraceType": "html",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import lit\n",
    "\n",
    "date_data = [\n",
    "    (\"USA\", \"MM/DD/YYYY\", \"YYYY/MM/DD\"),\n",
    "    (\"NL\", \"DD/MM/YYYY\", \"YYYY/MM/DD\"),\n",
    "    (\"IND\", \"DD/MM/YYYY\", \"YYYY/MM/DD\"),\n",
    "]\n",
    "date_columns = [\"Country_ID\", \"Original_Format\", \"Desired_Format\"]\n",
    "\n",
    "country_data = [\n",
    "    (\"INDIA\",\"IND\", 1),\n",
    "    (\"USA\",\"USA\", 2),\n",
    "    (\"Netherlands\",\"NL\", 3)\n",
    "]\n",
    "country_columns = [\"Country_Name\", \"Country_Code\", \"Country_ID\"]\n",
    "\n",
    "product_data = [\n",
    "    (89, 5001, \"Regular Gasoline\"),\n",
    "    (91, 5002, \"Premium Gasoline\"),\n",
    "    (94, 5003, \"Diesel\"),\n",
    "    (98, 5004, \"Premium Diesel\"),\n",
    "    (99, 5005, \"Super Premium Diesel\")\n",
    "]\n",
    "product_columns = [\"Local_Product_ID\", \"Global_Product_ID\", \"Global_Product_Category\"]\n",
    "\n",
    "site_data = [\n",
    "    # India (IN) Sites\n",
    "    (\"IND\", 1, 1001),\n",
    "    (\"IND\", 2, 1002),\n",
    "    (\"IND\", 3, 1003),\n",
    "    (\"IND\", 4, 1004),\n",
    "    (\"IND\", 5, 1005),\n",
    "    (\"IND\", 6, 1006),\n",
    "    (\"IND\", 7, 1007),\n",
    "    (\"IND\", 8, 1008),\n",
    "    (\"IND\", 9, 1009),\n",
    "    (\"IND\", 10, 1010),\n",
    "\n",
    "    # USA (US) Sites\n",
    "    (\"USA\", 1, 2001),\n",
    "    (\"USA\", 2, 2002),\n",
    "    (\"USA\", 3, 2003),\n",
    "    (\"USA\", 4, 2004),\n",
    "    (\"USA\", 5, 2005),\n",
    "    (\"USA\", 6, 2006),\n",
    "    (\"USA\", 7, 2007),\n",
    "    (\"USA\", 8, 2008),\n",
    "    (\"USA\", 9, 2009),\n",
    "    (\"USA\", 10, 2010),\n",
    "\n",
    "    # Netherlands (NL) Sites\n",
    "    (\"NL\", 1, 3001),\n",
    "    (\"NL\", 2, 3002),\n",
    "    (\"NL\", 3, 3003),\n",
    "    (\"NL\", 4, 3004),\n",
    "    (\"NL\", 5, 3005),\n",
    "    (\"NL\", 6, 3006),\n",
    "    (\"NL\", 7, 3007),\n",
    "    (\"NL\", 8, 3008),\n",
    "    (\"NL\", 9, 3009),\n",
    "    (\"NL\", 10, 3010)\n",
    "]\n",
    "site_columns = [\"Country_Code\", \"Local_Site_ID\", \"Global_Site_ID\"]\n",
    "\n",
    "global_volume_data = [\n",
    "    (\"gallons\", 3.78541, \"liters\"),\n",
    "    (\"liters\", 1.0, \"liters\"),\n",
    "    (\"quarts\", 0.946353, \"liters\"),\n",
    "    (\"pints\", 0.473176, \"liters\"),\n",
    "    (\"milliliters\", 0.001, \"liters\")\n",
    "]\n",
    "global_volume_columns = [\"Original_Unit\", \"Conversion_Factor\", \"Standard_Unit\"]\n",
    "global_vol_df = spark.createDataFrame(global_volume_data, global_volume_columns)\n",
    "global_vol_df = global_vol_df.withColumn(\n",
    "    \"Conversion_Factor\", col(\"Conversion_Factor\").cast(DoubleType())\n",
    ")\n",
    "global_Country_df = spark.createDataFrame(country_data, country_columns)\n",
    "global_product_df = spark.createDataFrame(product_data, product_columns)\n",
    "global_site_df = spark.createDataFrame(site_data, site_columns)\n",
    "global_dateformat_df = spark.createDataFrame(date_data,date_columns )\n",
    "\n",
    "# Display the Global Reference Tables\n",
    "print(\"Global Country Reference Table:\")\n",
    "global_Country_df.display()\n",
    "\n",
    "print(\"Global Product Reference Table:\")\n",
    "global_product_df.display()\n",
    "\n",
    "print(\"Global Site Reference Table:\")\n",
    "global_site_df.display()\n",
    "\n",
    "print(\"Global date fromat Reference Table:\")\n",
    "global_dateformat_df.display()\n",
    "\n",
    "print(\"Global volume Reference Table:\")\n",
    "global_vol_df.display()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c3348770-7037-4777-a35d-f9ed09bb8ebb",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "Command skipped",
       "errorTraceType": "html",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def convert_date(df):\n",
    "    # Directly use the date format based on Country_ID\n",
    "    df_converted_dates = df.withColumn(\n",
    "        \"Date\",\n",
    "        when(\n",
    "            col(\"Country_ID\") == \"USA\", to_date(col(\"Date\"), \"MM/dd/yyyy\")\n",
    "        ).when(\n",
    "            col(\"Country_ID\").isin(\"NL\", \"IND\"), to_date(col(\"Date\"), \"dd/MM/yyyy\")\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    return df_converted_dates\n",
    "\n",
    "def convert_data_types(df):\n",
    "    converted_df = df \\\n",
    "        .withColumn(\"Country_ID\", col(\"Country_ID\").cast(\"string\")) \\\n",
    "        .withColumn(\"Date\", col(\"Date\").cast(\"date\")) \\\n",
    "        .withColumn(\"Site_ID\", col(\"Site_ID\").cast(\"integer\")) \\\n",
    "        .withColumn(\"Product_ID\", col(\"Product_ID\").cast(\"integer\")) \\\n",
    "        .withColumn(\"Volume_Sold\", col(\"Volume_Sold\").cast(\"string\")) \\\n",
    "        .withColumn(\"load_timestamp\", expr(\"CAST(load_timestamp AS TIMESTAMP)\"))\n",
    "    \n",
    "    return converted_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c891f419-5207-40e1-a51f-0de029ba1740",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "Command skipped",
       "errorTraceType": "html",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, to_date, regexp_replace, round, when, trim, max as max_, lower\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.types import DoubleType\n",
    "\n",
    "#drop nulls\n",
    "null_replacement_list = ['null', 'NULL', 'NaN', 'n/a', 'N/A', 'none', 'None', 'undefined', 'missing', 'blank', ' ', 'UNKNOWN', 'Unknown', 'unknown', '-1']\n",
    "\n",
    "quarantine_df = combined_df.filter(\n",
    "    col(\"Volume_Sold\").isin(null_replacement_list) |\n",
    "    col(\"Date\").isin(null_replacement_list) |\n",
    "    col(\"Site_ID\").isin(null_replacement_list) |\n",
    "    col(\"Product_ID\").isin(null_replacement_list)\n",
    ")\n",
    "\n",
    "quarentine_delta_path = f\"abfss://{adls_container_name}@{adls_storage_account_name}.dfs.core.windows.net/quarentine_table\"\n",
    "\n",
    "quarantine_df.write.format(\"delta\").mode(\"overwrite\").save(quarentine_delta_path)\n",
    "\n",
    "#drop nulls\n",
    "null_replacement_list = ['null', 'NULL', 'NaN', 'n/a', 'N/A', 'none', 'None', 'undefined', 'missing', 'blank', ' ', 'UNKNOWN', 'Unknown', 'unknown', '-1']\n",
    "cleaned_df = combined_df.replace(null_replacement_list, None)\n",
    "# Trim whitespace\n",
    "cleaned_df = cleaned_df.select([trim(col(c)).alias(c) for c in cleaned_df.columns])\n",
    "critical_columns = [\"Site_ID\", \"Product_ID\", \"Volume_Sold\", \"Date\", \"load_timestamp\"]\n",
    "cleaned_df = cleaned_df.dropna(subset=critical_columns)\n",
    "\n",
    "# remove duplicate and take max volume sold\n",
    "window_spec = Window.partitionBy(\"Country_ID\", \"Date\", \"Site_ID\", \"Product_ID\").orderBy(desc(\"Volume_Sold\"))\n",
    "cleaned_df = cleaned_df.withColumn(\"row_num\", row_number().over(window_spec))\n",
    "cleaned_df = cleaned_df.filter(col(\"row_num\") == 1).drop(\"row_num\")\n",
    "\n",
    "# Display the result\n",
    "cleaned_df.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0b8d6994-8a5c-4816-940e-83b34e043e16",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "Command skipped",
       "errorTraceType": "html",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "bronze_delta_path = f\"abfss://{adls_container_name}@{adls_storage_account_name}.dfs.core.windows.net/cleaned_bronze_table\"\n",
    "\n",
    "cleaned_df.write.format(\"delta\").mode(\"overwrite\").save(bronze_delta_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6e9fa287-a820-4076-a53f-4565e16fc862",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "Command skipped",
       "errorTraceType": "html",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#data type conversions\n",
    "cleaned_df = convert_date(cleaned_df)\n",
    "cleaned_df = convert_data_types(cleaned_df)\n",
    "cleaned_df.printSchema()\n",
    "cleaned_df.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b114732d-aaae-4c90-889e-c80c238cf875",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "Command skipped",
       "errorTraceType": "html",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "cleaned_df = cleaned_df.withColumn(\n",
    "    \"Volume_Value\", regexp_replace(col(\"Volume_Sold\"), \" .*\", \"\").cast(DoubleType())\n",
    ").withColumn(\n",
    "    \"Volume_Unit\", lower(regexp_replace(col(\"Volume_Sold\"), \"^[0-9.]+ \", \"\"))\n",
    ")\n",
    "\n",
    "# Join with the reference table\n",
    "cleaned_df = cleaned_df.join(\n",
    "    global_vol_df,\n",
    "    lower(cleaned_df[\"Volume_Unit\"]) == lower(global_vol_df[\"Original_Unit\"]),\n",
    "    \"left\"\n",
    ").withColumn(\n",
    "    \"Volume_Sold_Liters\",\n",
    "    when(col(\"Conversion_Factor\").isNotNull(), round(col(\"Volume_Value\") * col(\"Conversion_Factor\"), 2))\n",
    "    .otherwise(col(\"Volume_Value\"))\n",
    ").select(\n",
    "    \"Country_ID\",\n",
    "    \"Date\",\n",
    "    \"Site_ID\",\n",
    "    \"Product_ID\",\n",
    "    \"Volume_Sold\",\n",
    "    \"load_timestamp\",\n",
    "    \"Volume_Sold_Liters\" \n",
    ")\n",
    "\n",
    "# Display the resulting DataFrame\n",
    "cleaned_df.printSchema()\n",
    "cleaned_df.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eaa593ca-3fba-40b4-953c-aedebe3886e5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "Command skipped",
       "errorTraceType": "html",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "cleaned_df = cleaned_df.join(global_Country_df, cleaned_df.Country_ID == global_Country_df.Country_Code, \"left\") \\\n",
    "                        .select(cleaned_df[\"*\"], global_Country_df[\"Country_ID\"].alias(\"Global_Country_ID\"))\n",
    "\n",
    "cleaned_df = cleaned_df.join(global_product_df, cleaned_df.Product_ID == global_product_df.Local_Product_ID, \"left\") \\\n",
    "                       .select(cleaned_df[\"*\"], \n",
    "                               global_product_df[\"Global_Product_ID\"], \n",
    "                               global_product_df[\"Global_Product_Category\"])\n",
    "\n",
    "cleaned_df.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "492840d6-e29f-4ad9-82b7-858ae048ccbf",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "Command skipped",
       "errorTraceType": "html",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "final_df = cleaned_df.join(global_site_df, (cleaned_df.Country_ID == global_site_df.Country_Code) & \n",
    "                                           (cleaned_df.Site_ID == global_site_df.Local_Site_ID), \"left\") \\\n",
    "                       .select(cleaned_df[\"*\"], global_site_df[\"Global_Site_ID\"])\n",
    "display(final_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f1d6df5d-cb0e-4363-87e5-eae14e7775f1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "Command skipped",
       "errorTraceType": "html",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType, DateType, TimestampType, LongType\n",
    "schema = StructType([\n",
    "    StructField(\"Country_ID\", StringType(), True),\n",
    "    StructField(\"Date\", DateType(), True),\n",
    "    StructField(\"Site_ID\", IntegerType(), True),\n",
    "    StructField(\"Product_ID\", IntegerType(), True),\n",
    "    StructField(\"Volume_Sold\", StringType(), True),\n",
    "    StructField(\"load_timestamp\", TimestampType(), True),\n",
    "    StructField(\"Volume_Sold_Liters\", DoubleType(), True),\n",
    "    StructField(\"Global_Country_ID\", LongType(), True),\n",
    "    StructField(\"Global_Product_ID\", LongType(), True),\n",
    "    StructField(\"Global_Product_Category\", StringType(), True),\n",
    "    StructField(\"Global_Site_ID\", LongType(), True)\n",
    "])\n",
    "\n",
    "# Read CSV from blob storage\n",
    "csv_path = f\"wasbs://{blob_container_name}@{blob_storage_account_name}.blob.core.windows.net/silver_prev_data.csv\"\n",
    "silver_df = spark.read.csv(csv_path, schema=schema, header=True, inferSchema=False)\n",
    "\n",
    "# Define Delta table path in ADLS\n",
    "delta_table_path = f\"abfss://{adls_container_name}@{adls_storage_account_name}.dfs.core.windows.net/delta/rbcisk\"\n",
    "\n",
    "# Write DataFrame to Delta table\n",
    "silver_df.write.format(\"delta\").mode(\"overwrite\").save(delta_table_path)\n",
    "\n",
    "silver_df.show(36602, truncate=False)\n",
    "num_rows = silver_df.count()\n",
    "print(f\"Number of rows: {num_rows}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "74184699-fd36-458a-9d10-c324d2803927",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "Command skipped",
       "errorTraceType": "html",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from delta.tables import DeltaTable  # Import DeltaTable\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, row_number, to_timestamp\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "# Assume spark is your SparkSession\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "# Ensure consistent data types\n",
    "final_df = final_df.withColumn(\"Country_Id\", col(\"Country_Id\").cast(\"string\")) \\\n",
    "                   .withColumn(\"Global_Country_ID\", col(\"Global_Country_ID\").cast(\"int\")) \\\n",
    "                   .withColumn(\"Date\", to_timestamp(col(\"Date\"))) \\\n",
    "                   .withColumn(\"Site_ID\", col(\"Site_ID\").cast(\"int\")) \\\n",
    "                   .withColumn(\"Global_Site_ID\", col(\"Global_Site_ID\").cast(\"int\")) \\\n",
    "                   .withColumn(\"Product_ID\", col(\"Product_ID\").cast(\"int\")) \\\n",
    "                   .withColumn(\"Global_Product_ID\", col(\"Global_Product_ID\").cast(\"int\")) \\\n",
    "                   .withColumn(\"Global_Product_Category\", col(\"Global_Product_Category\").cast(\"string\")) \\\n",
    "                   .withColumn(\"Volume_Sold\", col(\"Volume_Sold\").cast(\"string\")) \\\n",
    "                   .withColumn(\"Volume_Sold_Liters\", col(\"Volume_Sold_Liters\").cast(\"float\")) \\\n",
    "                   .withColumn(\"load_timestamp\", to_timestamp(col(\"load_timestamp\")))\n",
    "\n",
    "# Convert both DataFrames to Delta tables\n",
    "silver_df.write.format(\"delta\").mode(\"overwrite\").save(\"/tmp/silver_df\")\n",
    "final_df.write.format(\"delta\").mode(\"overwrite\").save(\"/tmp/final_df\")\n",
    "\n",
    "# Read them back as Delta tables\n",
    "silver_table = DeltaTable.forPath(spark, \"/tmp/silver_df\")\n",
    "final_table = DeltaTable.forPath(spark, \"/tmp/final_df\")\n",
    "\n",
    "# Create a temporary view of final_table with the latest records based on load_timestamp\n",
    "window_spec = Window.partitionBy(\"Country_Id\", \"Site_ID\", \"Product_ID\").orderBy(col(\"load_timestamp\").desc())\n",
    "\n",
    "latest_final_df = final_df.withColumn(\"rn\", row_number().over(window_spec)).filter(col(\"rn\") == 1).drop(\"rn\")\n",
    "\n",
    "latest_final_df.createOrReplaceTempView(\"latest_final_df\")\n",
    "\n",
    "# Perform SCD Type 1 Merge using Delta Lake\n",
    "silver_table.alias(\"s\").merge(\n",
    "    latest_final_df.alias(\"f\"),\n",
    "    \"s.Country_Id = f.Country_Id AND s.Site_ID = f.Site_ID AND s.Product_ID = f.Product_ID\"\n",
    ").whenMatchedUpdate(set={\n",
    "    \"Global_Country_ID\": col(\"f.Global_Country_ID\"),\n",
    "    \"Date\": col(\"f.Date\"),\n",
    "    \"Global_Site_ID\": col(\"f.Global_Site_ID\"),\n",
    "    \"Global_Product_ID\": col(\"f.Global_Product_ID\"),\n",
    "    \"Global_Product_Category\": col(\"f.Global_Product_Category\"),\n",
    "    \"Volume_Sold\": col(\"f.Volume_Sold\"),\n",
    "    \"Volume_Sold_Liters\": col(\"f.Volume_Sold_Liters\"),\n",
    "    \"load_timestamp\": col(\"f.load_timestamp\")\n",
    "}).whenNotMatchedInsert(values={\n",
    "    \"Country_Id\": col(\"f.Country_Id\"),\n",
    "    \"Global_Country_ID\": col(\"f.Global_Country_ID\"),\n",
    "    \"Date\": col(\"f.Date\"),\n",
    "    \"Site_ID\": col(\"f.Site_ID\"),\n",
    "    \"Global_Site_ID\": col(\"f.Global_Site_ID\"),\n",
    "    \"Product_ID\": col(\"f.Product_ID\"),\n",
    "    \"Global_Product_ID\": col(\"f.Global_Product_ID\"),\n",
    "    \"Global_Product_Category\": col(\"f.Global_Product_Category\"),\n",
    "    \"Volume_Sold\": col(\"f.Volume_Sold\"),\n",
    "    \"Volume_Sold_Liters\": col(\"f.Volume_Sold_Liters\"),\n",
    "    \"load_timestamp\": col(\"f.load_timestamp\")\n",
    "}).execute()\n",
    "\n",
    "# Verify the records in silver_df\n",
    "silver_table.toDF().orderBy(\"Country_Id\", \"Site_ID\", \"Product_ID\").show(36602, truncate=False)\n",
    "num_rows = silver_table.toDF().count()\n",
    "print(f\"Number of rows: {num_rows}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c167169f-5858-41db-9616-229373d0a489",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "04368fd4-af43-4793-a725-abed2c3d3793",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "Command skipped",
       "errorTraceType": "html",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.sql.functions import sha2, concat_ws\n",
    "primary_key_columns = [\"Country_Id\", \"Site_ID\", \"Product_ID\"]\n",
    "# Create a hash for both old and new data (excluding the primary key columns)\n",
    "old_df_with_hash = final_df.withColumn(\"hash\", sha2(concat_ws(\"||\", *[col for col in final_df.columns if col not in primary_key_columns]), 256))\n",
    "updated_df_with_hash = silver_table.toDF().withColumn(\"hash\", sha2(concat_ws(\"||\", *[col for col in silver_table.toDF().columns if col not in primary_key_columns]), 256))\n",
    "\n",
    "# Compare the hashes\n",
    "changed_records = old_df_with_hash.join(updated_df_with_hash, primary_key_columns, \"inner\").filter(old_df_with_hash[\"hash\"] != updated_df_with_hash[\"hash\"])\n",
    "\n",
    "changed_count = changed_records.count()\n",
    "print(f\"Number of records updated: {changed_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "51f631d8-073e-4d15-8ce6-16e09377ea84",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "Command skipped",
       "errorTraceType": "html",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Join old and updated dataframes on the primary keys\n",
    "primary_key_columns = [\"Country_Id\", \"Site_ID\", \"Product_ID\"]  # Replace with your actual key columns\n",
    "\n",
    "# Anti-join to find the rows that have changed (i.e., records in old_df that are not in updated_df)\n",
    "changed_records = final_df.join(silver_table.toDF(), primary_key_columns, \"leftanti\")\n",
    "\n",
    "changed_count = changed_records.count()\n",
    "print(f\"Number of records updated: {changed_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1ab0557a-d1b5-4b00-b5c2-cec82820324d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9c891dc0-0e30-43f6-ab24-9f0c2739f1df",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[4]: [FileInfo(path='dbfs:/FileStore/Silver_table_afterSCD1.csv', name='Silver_table_afterSCD1.csv', size=168821, modificationTime=1724257553000)]"
     ]
    }
   ],
   "source": [
    "dbutils.fs.ls(\"/FileStore/Silver_table_afterSCD1.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a10a022d-bc67-4ea3-9271-c3250f5a1e66",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------------------+-------------------+\n|      Date|Total_Volume_Sold_Liters|         Difference|\n+----------+------------------------+-------------------+\n|2024-07-31|      161441.81999999998|               null|\n|2024-08-01|      124797.12000000005|-36644.699999999924|\n|2024-08-02|      154608.25999999995| 29811.139999999898|\n|2024-08-03|      130559.22000000003| -24049.03999999992|\n|2024-08-04|               138356.21|  7796.989999999962|\n|2024-08-05|      120288.39999999998|-18067.810000000012|\n|2024-08-06|               135196.25|  14907.85000000002|\n+----------+------------------------+-------------------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, to_date, sum as _sum, lag\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "\n",
    "# Load the data from CSV\n",
    "file_path = \"dbfs:/FileStore/Silver_table_afterSCD1.csv\"  # Replace with your actual file path\n",
    "data_df = spark.read.option(\"header\", \"true\").csv(file_path)\n",
    "\n",
    "# Convert the 'Date' column to DateType\n",
    "data_df = data_df.withColumn(\"Date\", to_date(col(\"Date\"), \"yyyy-MM-dd\"))\n",
    "\n",
    "# Define the end date (today's date) and start date (one week before)\n",
    "end_date = \"2024-08-06\"\n",
    "start_date = \"2024-07-30\"\n",
    "\n",
    "# Filter the data for the last week\n",
    "filtered_df = data_df.filter((col(\"Date\") > start_date) & (col(\"Date\") <= end_date))\n",
    "\n",
    "# Summarize the data by date to calculate total Volume Sold\n",
    "weekly_summary_df = filtered_df.groupBy(\"Date\").agg(_sum(col(\"Volume_Sold_Liters\")).alias(\"Total_Volume_Sold_Liters\"))\n",
    "\n",
    "# Create a window specification to calculate the difference from the previous day\n",
    "window_spec = Window.orderBy(\"Date\")\n",
    "\n",
    "# Calculate the difference in sales compared to the previous day\n",
    "weekly_summary_df = weekly_summary_df.withColumn(\"Difference\", col(\"Total_Volume_Sold_Liters\") - lag(col(\"Total_Volume_Sold_Liters\")).over(window_spec))\n",
    "\n",
    "# Show the result\n",
    "weekly_summary_df.orderBy(\"Date\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bcd55fd7-6539-499e-992f-adb50ddfc1aa",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------+--------------+---------------+----------+\n|Today_Date|Today_Sales|Last_Week_Date|Last_Week_Sales|Difference|\n+----------+-----------+--------------+---------------+----------+\n|2024-08-06|  135196.25|    2024-07-30|      135196.25|       0.0|\n+----------+-----------+--------------+---------------+----------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, to_date, sum as _sum, date_sub\n",
    "\n",
    "# Load the data from CSV\n",
    "file_path = \"dbfs:/FileStore/Silver_table_afterSCD1.csv\"  # Replace with your actual file path\n",
    "data_df = spark.read.option(\"header\", \"true\").csv(file_path)\n",
    "\n",
    "# Convert the 'Date' column to DateType\n",
    "data_df = data_df.withColumn(\"Date\", to_date(col(\"Date\"), \"yyyy-MM-dd\"))\n",
    "\n",
    "# Summarize the data by date to calculate total Volume Sold\n",
    "daily_sales_df = data_df.groupBy(\"Date\").agg(_sum(col(\"Volume_Sold_Liters\")).alias(\"Total_Volume_Sold_Liters\"))\n",
    "\n",
    "# Create a DataFrame for today's sales\n",
    "today_sales_df = daily_sales_df.filter(col(\"Date\") == \"2024-08-06\").select(col(\"Date\").alias(\"Today_Date\"), col(\"Total_Volume_Sold_Liters\").alias(\"Today_Sales\"))\n",
    "\n",
    "# Create a DataFrame for last week's same day sales\n",
    "last_week_sales_df = daily_sales_df.withColumn(\"Last_Week_Date\", date_sub(col(\"Date\"), 7)).filter(col(\"Date\") == \"2024-08-06\").select(col(\"Last_Week_Date\").alias(\"Last_Week_Date\"), col(\"Total_Volume_Sold_Liters\").alias(\"Last_Week_Sales\"))\n",
    "\n",
    "# Join today's sales with last week's same day sales\n",
    "comparison_df = today_sales_df.crossJoin(last_week_sales_df)\n",
    "\n",
    "# Calculate the difference\n",
    "comparison_df = comparison_df.withColumn(\"Difference\", col(\"Today_Sales\") - col(\"Last_Week_Sales\"))\n",
    "\n",
    "# Show the result\n",
    "comparison_df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5c87384c-0bbe-4d25-b619-03581650a562",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8db11dea-2678-47f2-bcb1-b262e796b368",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jerry\n"
     ]
    }
   ],
   "source": [
    "d = {1: \"Raj\", 2:\"Jerry\"}\n",
    "l = []\n",
    "for i in d.keys():\n",
    "    l.append(i)\n",
    "print(d[max(l)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b68fa2d5-61c9-4c6b-9b7d-20064ae2275c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Gold Table(done in snowflake)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9ef49f10-03d3-4334-b3a3-0f1c9ab543a2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "create database agg;\n",
    "\n",
    "-- Switch to the appropriate database\n",
    "USE DATABASE agg;\n",
    "\n",
    "CREATE OR REPLACE STAGE my_stage2;\n",
    "\n",
    "select * from silver_table;\n",
    "\n",
    "-- Set the session parameter for timestamp input format\n",
    "ALTER SESSION SET TIMESTAMP_INPUT_FORMAT = 'YYYY-MM-DD\"T\"HH24:MI:SS.FF3TZHTZM';\n",
    "\n",
    "-- Define the file format with correct timestamp handling\n",
    "CREATE OR REPLACE FILE FORMAT my_csv_format\n",
    "  TYPE = 'CSV'\n",
    "  FIELD_OPTIONALLY_ENCLOSED_BY = '\"'\n",
    "  SKIP_HEADER = 1\n",
    "  NULL_IF = ('NULL', 'null')\n",
    "  DATE_FORMAT = 'YYYY-MM-DD'\n",
    "  TIME_FORMAT = 'HH24:MI:SS.FF3TZHTZM';\n",
    "\n",
    "-- Create or replace the stage\n",
    "CREATE OR REPLACE STAGE my_stage2\n",
    "  FILE_FORMAT = my_csv_format;\n",
    "\n",
    "-- Create the table\n",
    "CREATE OR REPLACE TABLE silver_table (\n",
    "    Country_ID VARCHAR,\n",
    "    Date DATE,\n",
    "    Site_ID VARCHAR,\n",
    "    Product_ID VARCHAR,\n",
    "    Volume_Sold VARCHAR,\n",
    "    load_timestamp TIMESTAMP_TZ,\n",
    "    Volume_Sold_Liters DOUBLE,\n",
    "    Global_Country_ID VARCHAR,\n",
    "    Global_Product_ID VARCHAR,\n",
    "    Global_Product_Category VARCHAR,\n",
    "    Global_Site_ID VARCHAR,\n",
    "    update_timestamp TIMESTAMP_TZ\n",
    ");\n",
    "\n",
    "-- List files in the stage (optional, for debugging)\n",
    "LIST @my_stage2;\n",
    "\n",
    "-- Load data from the stage into the table\n",
    "COPY INTO silver_table\n",
    "FROM @my_stage2/Silver_table_afterSCD1.csv\n",
    "FILE_FORMAT = my_csv_format\n",
    "ON_ERROR = 'CONTINUE';\n",
    "\n",
    "-- Validate data loading and format errors\n",
    "COPY INTO silver_table\n",
    "FROM @my_stage2/Silver_table_afterSCD1.csv\n",
    "FILE_FORMAT = my_csv_format\n",
    "VALIDATION_MODE = 'RETURN_ERRORS';\n",
    "\n",
    "\n",
    "\n",
    "select * from silver_table;\n",
    "\n",
    "\n",
    "\n",
    "CREATE OR REPLACE TABLE gold_table (\n",
    "    Country_ID STRING,\n",
    "    Date DATE,\n",
    "    Site_ID STRING,\n",
    "    Product_ID STRING,\n",
    "    Volume_Sold STRING,\n",
    "    load_timestamp TIMESTAMP_TZ,\n",
    "    Volume_Sold_Liters DOUBLE,\n",
    "    Global_Country_ID STRING,\n",
    "    Global_Product_ID STRING,\n",
    "    Global_Product_Category STRING,\n",
    "    Global_Site_ID STRING,\n",
    "    Volume_Sold_Last_Week DOUBLE,\n",
    "    Volume_Sold_Last_Month DOUBLE,\n",
    "    Volume_Sold_Last_Year DOUBLE,\n",
    "    Pct_Change_Last_Week DOUBLE,\n",
    "    Pct_Change_Last_Month DOUBLE,\n",
    "    Pct_Change_Last_Year DOUBLE,\n",
    "    Rolling_7_Day_Sum DOUBLE,\n",
    "    Pct_Change_Rolling_7_Day_Sum DOUBLE\n",
    ");\n",
    "\n",
    "\n",
    "INSERT INTO gold_table\n",
    "SELECT\n",
    "    s.Country_ID,\n",
    "    s.Date,\n",
    "    s.Site_ID,\n",
    "    s.Product_ID,\n",
    "    s.Volume_Sold,\n",
    "    s.load_timestamp,\n",
    "    s.Volume_Sold_Liters,\n",
    "    s.Global_Country_ID,\n",
    "    s.Global_Product_ID,\n",
    "    s.Global_Product_Category,\n",
    "    s.Global_Site_ID,\n",
    "    COALESCE(lw.Volume_Sold_Liters, 0) AS Volume_Sold_Last_Week,\n",
    "    COALESCE(lm.Volume_Sold_Liters, 0) AS Volume_Sold_Last_Month,\n",
    "    COALESCE(ly.Volume_Sold_Liters, 0) AS Volume_Sold_Last_Year,\n",
    "    CASE \n",
    "        WHEN lw.Volume_Sold_Liters IS NULL OR lw.Volume_Sold_Liters = 0 THEN NULL\n",
    "        ELSE (s.Volume_Sold_Liters - lw.Volume_Sold_Liters) / lw.Volume_Sold_Liters * 100\n",
    "    END AS Pct_Change_Last_Week,\n",
    "    CASE \n",
    "        WHEN lm.Volume_Sold_Liters IS NULL OR lm.Volume_Sold_Liters = 0 THEN NULL\n",
    "        ELSE (s.Volume_Sold_Liters - lm.Volume_Sold_Liters) / lm.Volume_Sold_Liters * 100\n",
    "    END AS Pct_Change_Last_Month,\n",
    "    CASE \n",
    "        WHEN ly.Volume_Sold_Liters IS NULL OR ly.Volume_Sold_Liters = 0 THEN NULL\n",
    "        ELSE (s.Volume_Sold_Liters - ly.Volume_Sold_Liters) / ly.Volume_Sold_Liters * 100\n",
    "    END AS Pct_Change_Last_Year,\n",
    "    COALESCE(rs.Rolling_7_Day_Sum, 0) AS Rolling_7_Day_Sum,\n",
    "    CASE \n",
    "        WHEN rs.Last_Rolling_Sum IS NULL OR rs.Last_Rolling_Sum = 0 THEN NULL\n",
    "        ELSE (rs.Rolling_7_Day_Sum - rs.Last_Rolling_Sum) / rs.Last_Rolling_Sum * 100\n",
    "    END AS Pct_Change_Rolling_7_Day_Sum\n",
    "FROM\n",
    "    silver_table s\n",
    "LEFT JOIN \n",
    "    silver_table lw \n",
    "    ON s.Country_ID = lw.Country_ID \n",
    "    AND s.Site_ID = lw.Site_ID \n",
    "    AND s.Product_ID = lw.Product_ID \n",
    "    AND s.Date = DATEADD('day', 7, lw.Date)\n",
    "LEFT JOIN \n",
    "    silver_table lm \n",
    "    ON s.Country_ID = lm.Country_ID \n",
    "    AND s.Site_ID = lm.Site_ID \n",
    "    AND s.Product_ID = lm.Product_ID \n",
    "    AND s.Date = DATEADD('day', 30, lm.Date)\n",
    "LEFT JOIN \n",
    "    silver_table ly \n",
    "    ON s.Country_ID = ly.Country_ID \n",
    "    AND s.Site_ID = ly.Site_ID \n",
    "    AND s.Product_ID = ly.Product_ID \n",
    "    AND s.Date = DATEADD('year', 1, ly.Date)\n",
    "LEFT JOIN (\n",
    "    SELECT\n",
    "        Country_ID,\n",
    "        Site_ID,\n",
    "        Product_ID,\n",
    "        Date,\n",
    "        SUM(Volume_Sold_Liters) AS Rolling_7_Day_Sum,\n",
    "        LAG(SUM(Volume_Sold_Liters)) OVER (PARTITION BY Country_ID, Site_ID, Product_ID ORDER BY Date) AS Last_Rolling_Sum\n",
    "    FROM\n",
    "        (SELECT\n",
    "            Country_ID,\n",
    "            Site_ID,\n",
    "            Product_ID,\n",
    "            Date,\n",
    "            SUM(Volume_Sold_Liters) OVER (PARTITION BY Country_ID, Site_ID, Product_ID ORDER BY Date ROWS BETWEEN 6 PRECEDING AND CURRENT ROW) AS Volume_Sold_Liters\n",
    "         FROM\n",
    "            silver_table\n",
    "        ) AS inner_rs\n",
    "    GROUP BY\n",
    "        Country_ID,\n",
    "        Site_ID,\n",
    "        Product_ID,\n",
    "        Date\n",
    ") rs\n",
    "ON s.Country_ID = rs.Country_ID \n",
    "AND s.Site_ID = rs.Site_ID \n",
    "AND s.Product_ID = rs.Product_ID \n",
    "AND s.Date = rs.Date;\n",
    "\n",
    "select* from gold_table;\n",
    "\n",
    "SELECT COUNT(*) AS row_count\n",
    "FROM gold_table;\n",
    "\n",
    "SELECT COUNT(*) AS row_count\n",
    "FROM silver_table;\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Real Time Project",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
