{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d64e8a73-8ba3-41a4-9ef6-f9d49b9aa29f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python interpreter will be restarted.\nCollecting pytest\n  Downloading pytest-8.2.2-py3-none-any.whl (339 kB)\nCollecting pluggy<2.0,>=1.5\n  Downloading pluggy-1.5.0-py3-none-any.whl (20 kB)\nRequirement already satisfied: packaging in /databricks/python3/lib/python3.9/site-packages (from pytest) (21.0)\nCollecting iniconfig\n  Downloading iniconfig-2.0.0-py3-none-any.whl (5.9 kB)\nRequirement already satisfied: tomli>=1 in /databricks/python3/lib/python3.9/site-packages (from pytest) (2.0.1)\nCollecting exceptiongroup>=1.0.0rc8\n  Downloading exceptiongroup-1.2.2-py3-none-any.whl (16 kB)\nRequirement already satisfied: pyparsing>=2.0.2 in /databricks/python3/lib/python3.9/site-packages (from packaging->pytest) (3.0.4)\nInstalling collected packages: pluggy, iniconfig, exceptiongroup, pytest\nSuccessfully installed exceptiongroup-1.2.2 iniconfig-2.0.0 pluggy-1.5.0 pytest-8.2.2\nPython interpreter will be restarted.\n"
     ]
    }
   ],
   "source": [
    "pip install pytest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "12cb11a1-813d-4d71-9670-d6cf196880fe",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a33f07f6-7acb-41eb-ba02-438e03568c46",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Unit Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a21b014f-45e7-449f-8b36-0e3f376f74ed",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import pytest\n",
    "\n",
    "# Initialize SparkSession for testing\n",
    "spark = SparkSession.builder.master(\"local\").appName(\"Test\").getOrCreate()\n",
    "\n",
    "# Sample function to test\n",
    "def process_data(df):\n",
    "    return df.filter(df['value'] > 10)\n",
    "def test_process_data():\n",
    "    sample_data = [(1, 5), (2, 15), (3, 25)]\n",
    "    columns = ['id', 'value']\n",
    "    df = spark.createDataFrame(sample_data, columns)\n",
    "    result = process_data(df)\n",
    "    \n",
    "    # Detailed assertions with messages\n",
    "    try:\n",
    "        assert result.count() == 2, f\"Expected 2 rows, but got {result.count()}\"\n",
    "        assert result.collect()[0]['value'] == 15, f\"Expected first row value to be 15, but got {result.collect()[0]['value']}\"\n",
    "        assert result.collect()[1]['value'] == 25, f\"Expected second row value to be 25, but got {result.collect()[1]['value']}\"\n",
    "        print(\"Test passed\")\n",
    "    except AssertionError as e:\n",
    "        print(\"Test failed\")\n",
    "        print(e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "04a05ff7-e253-4ba4-b36b-a2e7365ae182",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test passed\n"
     ]
    }
   ],
   "source": [
    "# Run the test\n",
    "test_process_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "726949d9-6ddb-4b3e-9f9a-a8d766d26e85",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e0054c22-786c-4b3a-8188-fefb8fa0dca5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "512e43eb-9280-4005-8069-ff5cf0283584",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Integration Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d3b96a00-4b20-4f34-b711-c0ec6e6a3b86",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed DataFrame:\n+---+-------+---+\n| id|   name|age|\n+---+-------+---+\n|  2|    Bob| 30|\n|  3|Charlie| 28|\n+---+-------+---+\n\nTest passed\n"
     ]
    }
   ],
   "source": [
    "import pytest\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Initialize SparkSession for testing\n",
    "spark = SparkSession.builder.master(\"local\").appName(\"Test\").getOrCreate()\n",
    "\n",
    "# Sample function to test\n",
    "def process_data(df):\n",
    "    return df.filter(df['age'] > 25)\n",
    "\n",
    "def test_integration():\n",
    "    # Mock data setup\n",
    "    sample_data = [(1, 'Alice', 23), (2, 'Bob', 30), (3, 'Charlie', 28)]\n",
    "    df = spark.createDataFrame(sample_data, ['id', 'name', 'age'])\n",
    "    \n",
    "    # Example integration process\n",
    "    processed_df = process_data(df)\n",
    "    \n",
    "    # Print the processed DataFrame for inspection\n",
    "    print(\"Processed DataFrame:\")\n",
    "    processed_df.show()\n",
    "    \n",
    "    try:\n",
    "        assert processed_df.count() == 2, f\"Expected 2 rows, but got {processed_df.count()}\"\n",
    "        assert processed_df.collect()[0]['name'] == 'Bob', f\"Expected name 'Bob', but got {processed_df.collect()[0]['name']}\"\n",
    "        print(\"Test passed\")\n",
    "    except AssertionError as e:\n",
    "        print(\"Test failed\")\n",
    "        print(e)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    test_integration()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "80516ead-8891-4ae5-a2eb-f6ac25bef059",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "aa2287c5-4297-4387-a042-32d343c3a565",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "End-to-End Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a3d50330-e7b6-4f60-82b9-22e5f7051d3e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "test_data = [(1, 5), (2, 15), (3, 25), (4, 8), (5, 20)]\n",
    "columns = ['id', 'value']\n",
    "df_test = spark.createDataFrame(test_data, columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "aec3b853-50ce-4e89-8ac2-54a21902c75f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Assuming 'data_pipeline' is your notebook name\n",
    "notebook_params = {'input': df_test.toPandas().to_csv()}  # Convert DataFrame to CSV string for input\n",
    "dbutils.notebook.run('Testing', 60, notebook_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8cb64d22-a80a-4681-8b9f-b876c6d0c490",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Test a Notebook Using Pytest and nbval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c4f72d99-0b84-4c96-b274-b8920c07e4a6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "pip install pytest nbval\n",
    "# test_notebook.py\n",
    "\n",
    "import pytest\n",
    "\n",
    "@pytest.mark.parametrize('notebook_path', ['Testing.ipynb'])\n",
    "def test_notebook(notebook_path):\n",
    "    \"\"\"\n",
    "    Basic test to validate a notebook using nbval.\n",
    "    \"\"\"\n",
    "    import nbval\n",
    "    nbval.validate(notebook_path)\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Testing",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
